{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d568d29-9d24-436c-8cd1-c653c1a5d19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All preparations finished\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import densenet121\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def get_dataloaders(batch_size=128):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "def build_model(num_classes=10):\n",
    "    model = densenet121(weights=None) \n",
    "\n",
    "    num_features = model.classifier.in_features\n",
    "    model.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    # CIFAR-10 是 32x32，小于 DenseNet 预期的 224x224\n",
    "    # 所以第一个 conv 层 kernel=7 → kernel=3, stride=2 → stride=1\n",
    "    model.features.conv0 = nn.Conv2d(\n",
    "        3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "    return model\n",
    "\n",
    "def int_to_bin_str(val, bits):\n",
    "    \"\"\"Convert signed integer to two's complement binary string.\"\"\"\n",
    "    v = int(round(float(val)))\n",
    "    mask = (1 << bits) - 1\n",
    "    if v < 0:\n",
    "        v = (1 << bits) + v \n",
    "    v = v & mask\n",
    "    return format(v, '0{}b'.format(bits))\n",
    "\n",
    "# ==========================================\n",
    "# 2. 硬件仿真与导出类\n",
    "# ==========================================\n",
    "# class HardwareExporter:\n",
    "#     def __init__(self, layer, input_tensor, bit_width=4, array_size=8):\n",
    "#         \"\"\"\n",
    "#         layer: 训练好的 Conv2d 层\n",
    "#         input_tensor: 输入该层的 tensor (batch=1)\n",
    "#         bit_width: 量化比特数\n",
    "#         array_size: 脉动阵列大小 (8x8)\n",
    "#         \"\"\"\n",
    "#         self.layer = layer\n",
    "#         self.input = input_tensor.detach()\n",
    "#         self.bits = bit_width\n",
    "#         self.arr_size = array_size\n",
    "        \n",
    "#         # 获取量化参数 (假设使用对称量化，或者如果有特定alpha可在此修改)\n",
    "#         # 这里模拟计算 Scale，保证数据在 int 范围内\n",
    "#         self.act_max = self.input.abs().max()\n",
    "#         self.wgt_max = self.layer.weight.data.abs().max()\n",
    "        \n",
    "#         self.scale_a = (2**(self.bits-1) - 1) / self.act_max\n",
    "#         self.scale_w = (2**(self.bits-1) - 1) / self.wgt_max\n",
    "        \n",
    "#         print(f\"Quantization Info: Act_Scale={self.scale_a:.4f}, Wgt_Scale={self.scale_w:.4f}\")\n",
    "\n",
    "#     def get_quantized_int(self):\n",
    "#         \"\"\"获取量化后的整数 Tensor\"\"\"\n",
    "#         # Quantize Input\n",
    "#         inp_int = torch.round(self.input * self.scale_a).clamp(-(2**(self.bits-1)), 2**(self.bits-1)-1)\n",
    "        \n",
    "#         # Quantize Weight\n",
    "#         wgt_int = torch.round(self.layer.weight.data * self.scale_w).clamp(-(2**(self.bits-1)), 2**(self.bits-1)-1)\n",
    "        \n",
    "#         return inp_int, wgt_int\n",
    "\n",
    "#     def run_software_verification(self, kij_idx=0):\n",
    "#         \"\"\"\n",
    "#         纯软件模拟硬件行为，计算预期输出结果。\n",
    "#         仅计算 Slice 出的 8x8 区域。\n",
    "#         kij_idx: 3x3卷积核中的第几个点 (0~8)\n",
    "#         \"\"\"\n",
    "#         inp_int, wgt_int = self.get_quantized_int()\n",
    "        \n",
    "#         # 1. Slice: 取出 Input Channel 0-7, Output Channel 0-7\n",
    "#         # Input shape: [1, 64, 32, 32] -> Slice -> [1, 8, 32, 32]\n",
    "#         inp_slice = inp_int[:, :self.arr_size, :, :]\n",
    "        \n",
    "#         # Weight shape: [64, 64, 3, 3] -> Slice -> [8, 8, 3, 3] (Out, In, k, k)\n",
    "#         wgt_slice = wgt_int[:self.arr_size, :self.arr_size, :, :]\n",
    "        \n",
    "#         # 2. 模拟 Padding\n",
    "#         padding = self.layer.padding[0]\n",
    "#         inp_pad = F.pad(inp_slice, (padding, padding, padding, padding)) # [1, 8, 34, 34]\n",
    "        \n",
    "#         # 3. 提取特定 kij 对应的权重 [8, 8]\n",
    "#         # kij_idx 映射到 (ki, kj): 0->(0,0), 1->(0,1)... 4->(1,1 Center)\n",
    "#         ki = kij_idx // 3\n",
    "#         kj = kij_idx % 3\n",
    "#         w_matrix = wgt_slice[:, :, ki, kj] # [8, 8] -> (Out_Ch, In_Ch)\n",
    "        \n",
    "#         # 4. Flatten Input (按照 Im2Col 或 简单的 Time Series)\n",
    "#         # 硬件通常读取 Flatten 后的输入流\n",
    "#         # inp_pad shape: [1, 8, H_pad, W_pad] -> [8, Total_Pixels]\n",
    "#         inp_stream = inp_pad.squeeze(0).reshape(self.arr_size, -1)\n",
    "        \n",
    "#         # 5. 矩阵乘法模拟 (Systolic Array 核心逻辑: Weight * Input)\n",
    "#         # Output [8, Time] = Weight [8, 8] @ Input [8, Time]\n",
    "#         output_int = torch.matmul(w_matrix, inp_stream)\n",
    "        \n",
    "#         print(f\"Software Verification (kij={kij_idx}): Output Shape {output_int.shape}\")\n",
    "#         print(f\"Sample Output (First 5 values of Ch0): {output_int[0, :5].tolist()}\")\n",
    "        \n",
    "#         return inp_stream, w_matrix, output_int\n",
    "        \n",
    "#     def calculate_final_output(self, prefix=\"test\"):\n",
    "#         \"\"\"\n",
    "#         计算并导出 9 个 kij Partial Sums 累加后的最终输出，\n",
    "#         并应用 Bias 和 Activation Function。\n",
    "#         \"\"\"\n",
    "#         final_output = None\n",
    "        \n",
    "#         # 1. 累加 9 个 kij 的 Partial Sums\n",
    "#         print(\"Calculating final accumulated output...\")\n",
    "        \n",
    "#         for kij_idx in range(9):\n",
    "#             # run_software_verification 已经返回了量化后的 W * A 结果 (Partial Sum)\n",
    "#             inp_stream, w_matrix, current_partial_sum = self.run_software_verification(kij_idx)\n",
    "            \n",
    "#             # 累加 Partial Sums\n",
    "#             if final_output is None:\n",
    "#                 final_output = current_partial_sum\n",
    "#             else:\n",
    "#                 final_output += current_partial_sum\n",
    "                \n",
    "#         # 2. 处理 Bias 和 Activation Function\n",
    "        \n",
    "#         # 获取 Bias (DenseNet 121 for CIFAR-10 通常是 Conv + BN，所以 Conv 层无 Bias)\n",
    "#         bias = self.layer.bias\n",
    "#         if bias is not None:\n",
    "#             # Bias 是 [Out_Ch] 维度。final_output 是 [Out_Ch, Time]\n",
    "#             # 需要将 bias 扩展为 [Out_Ch, Time]\n",
    "#             bias_slice = bias[:self.arr_size].unsqueeze(1).expand_as(final_output)\n",
    "#             final_output += bias_slice\n",
    "#             print(\"Bias applied.\")\n",
    "#         else:\n",
    "#             # 检查 DenseNet 架构：通常 Conv 后面紧跟 BatchNorm，所以 Conv 没有 Bias。\n",
    "#             # 如果是这种情况，需要确保在 Verilog 端或后续层处理 BatchNorm。\n",
    "#             print(\"Note: Target layer has no bias. Assuming BatchNorm is handled externally/later.\")\n",
    "        \n",
    "#         # 3. 应用 Activation Function (DenseNet 的 Conv2 后面通常是 ReLU)\n",
    "#         # 在 Verilog 验证中，通常只验证 MAC/累加部分。但为了软件对比的完整性，我们应用 ReLU。\n",
    "        \n",
    "#         # Clamp 到 4-bit 范围 (可选，取决于硬件的输出位宽和后续层处理)\n",
    "#         # final_output = final_output.clamp(-(2**(self.bits-1)), 2**(self.bits-1)-1)\n",
    "        \n",
    "#         # 转换为 16-bit 整数 (假设累加结果位宽增加到 16-bit)\n",
    "#         final_output_int = torch.round(final_output)\n",
    "#         final_output_int = torch.nn.functional.relu(final_output_int)\n",
    "    \n",
    "#         # 4. 导出 Final Output\n",
    "#         out_filename = f\"{prefix}_final_output_ref.txt\"\n",
    "#         with open(out_filename, 'w') as f:\n",
    "#             f.write('# [Final Accumulated Output - Sum of 9 Partial Sums + Bias (if present)] Bits: Row7(MSB)..Row0(LSB) #\\n')\n",
    "            \n",
    "#             # 同样按照 Time Major, Row (Out Ch) High to Low 导出\n",
    "#             for t in range(final_output_int.shape[1]):\n",
    "#                 line_bin = \"\"\n",
    "#                 # Rows (Output Channels) 7 down to 0\n",
    "#                 for r in range(final_output_int.shape[0] - 1, -1, -1):\n",
    "#                     val = final_output_int[r, t].item()\n",
    "#                     # 累加后的结果通常需要更大的位宽 (例如 16 bit)\n",
    "#                     line_bin += int_to_bin_str(val, 16) \n",
    "#                 f.write(line_bin + '\\n')\n",
    "                \n",
    "#         print(f\"Exported Final Output Reference: {out_filename}\")\n",
    "#         return final_output_int\n",
    "        \n",
    "#     def export_files(self, kij_idx=0, prefix=\"test\"):\n",
    "#         \"\"\"导出 .txt 文件\"\"\"\n",
    "#         inp_stream, w_matrix, out_ref = self.run_software_verification(kij_idx)\n",
    "        \n",
    "#         # === 1. 导出 Activation (Input) ===\n",
    "#         # 格式: #time0row7...time0row0#\n",
    "#         # inp_stream shape: [8 (Rows), Time]\n",
    "#         act_filename = f\"{prefix}_activation_kij{kij_idx}.txt\"\n",
    "#         with open(act_filename, 'w') as f:\n",
    "#             f.write('# [Time Step Major] Each line is one time step. Bits: Row7(MSB)..Row0(LSB) #\\n')\n",
    "#             time_steps = inp_stream.shape[1]\n",
    "#             rows = inp_stream.shape[0] # 8\n",
    "            \n",
    "#             for t in range(time_steps):\n",
    "#                 line_bin = \"\"\n",
    "#                 # 注意：硬件通常要求高位在左 (Row 7)，低位在右 (Row 0)\n",
    "#                 for r in range(rows - 1, -1, -1): \n",
    "#                     val = inp_stream[r, t].item()\n",
    "#                     line_bin += int_to_bin_str(val, self.bits)\n",
    "#                 f.write(line_bin + '\\n')\n",
    "#         print(f\"Exported: {act_filename}\")\n",
    "\n",
    "#         # === 2. 导出 Weight ===\n",
    "#         # 格式: #col0row7...col0row0# (假设列代表Output Ch, 行代表Input Ch)\n",
    "#         # w_matrix shape: [8 (Out/Col), 8 (In/Row)]\n",
    "#         wgt_filename = f\"{prefix}_weight_kij{kij_idx}.txt\"\n",
    "#         with open(wgt_filename, 'w') as f:\n",
    "#             f.write('# [Col Major] Each line is one Output Channel (Col). Bits: Row7(InCh7)..Row0(InCh0) #\\n')\n",
    "#             num_cols = w_matrix.shape[0] # Output Channels\n",
    "#             num_rows = w_matrix.shape[1] # Input Channels\n",
    "            \n",
    "#             # 你的 Reference 代码(HW_Code6) 似乎并没有遍历 Col，而是直接取了一个 Tile。\n",
    "#             # 通常 Systolic Array Weight Loading 是一列一列 Load 或者一次性 Load。\n",
    "#             # 这里我们假设文件的一行对应 Array 的一列 (One Output Channel's weights across inputs)\n",
    "            \n",
    "#             for c in range(num_cols): # Output Channel Loop\n",
    "#                 line_bin = \"\"\n",
    "#                 for r in range(num_rows - 1, -1, -1): # Input Channel Loop (High to Low)\n",
    "#                     val = w_matrix[c, r].item()\n",
    "#                     line_bin += int_to_bin_str(val, self.bits)\n",
    "#                 f.write(line_bin + '\\n')\n",
    "#         print(f\"Exported: {wgt_filename}\")\n",
    "        \n",
    "#         # === 3. 导出 Golden Output (用于对比 Verilog 输出) ===\n",
    "#         out_filename = f\"{prefix}_output_ref_kij{kij_idx}.txt\"\n",
    "#         with open(out_filename, 'w') as f:\n",
    "#             # 同样按照 Time Major, Row (Out Ch) High to Low 导出\n",
    "#             for t in range(out_ref.shape[1]):\n",
    "#                 line_bin = \"\"\n",
    "#                 for r in range(out_ref.shape[0] - 1, -1, -1):\n",
    "#                     val = out_ref[r, t].item()\n",
    "#                     # Output 通常位宽会变大 (例如 16bit)，防止溢出\n",
    "#                     line_bin += int_to_bin_str(val, 16) \n",
    "#                 f.write(line_bin + '\\n')\n",
    "#         print(f\"Exported: {out_filename}\")\n",
    "\n",
    "class HardwareExporter:\n",
    "    # ... (其他方法如 __init__, int_to_bin_str, get_quantized_int 保持不变) ...\n",
    "    \n",
    "    def __init__(self, layer, input_tensor, bit_width=4, array_size=8):\n",
    "        \"\"\"\n",
    "        layer: 训练好的 Conv2d 层\n",
    "        input_tensor: 输入该层的 tensor (batch=1)\n",
    "        bit_width: 量化比特数\n",
    "        array_size: 脉动阵列大小 (8x8)\n",
    "        \"\"\"\n",
    "        self.layer = layer\n",
    "        self.input = input_tensor.detach().cpu()\n",
    "        self.bits = bit_width\n",
    "        self.arr_size = array_size\n",
    "        \n",
    "        # 获取量化参数\n",
    "        self.act_max = self.input.abs().max()\n",
    "        self.wgt_max = self.layer.weight.data.abs().max()\n",
    "        \n",
    "        self.scale_a = (2**(self.bits-1) - 1) / self.act_max\n",
    "        self.scale_w = (2**(self.bits-1) - 1) / self.wgt_max\n",
    "        \n",
    "        print(f\"Quantization Info: Act_Scale={self.scale_a:.4f}, Wgt_Scale={self.scale_w:.4f}\")\n",
    "\n",
    "    def int_to_bin_str(self, val, bits):\n",
    "        \"\"\"将整数转换为补码形式的二进制字符串\"\"\"\n",
    "        val = int(round(val))\n",
    "        if val < 0:\n",
    "            val = (1 << bits) + val\n",
    "        mask = (1 << bits) - 1\n",
    "        val = val & mask\n",
    "        return f\"{val:0{bits}b}\"\n",
    "\n",
    "    def get_quantized_int(self):\n",
    "        \"\"\"获取量化后的整数输入和权重\"\"\"\n",
    "        inp_int = torch.round(self.input * self.scale_a).clamp(-(2**(self.bits-1)), 2**(self.bits-1)-1)\n",
    "        wgt_int = torch.round(self.layer.weight.data * self.scale_w).clamp(-(2**(self.bits-1)), 2**(self.bits-1)-1)\n",
    "        return inp_int, wgt_int\n",
    "\n",
    "    def run_software_verification(self, kij_idx=0, input_tile_size=6, output_tile_size=4):\n",
    "        \"\"\"\n",
    "        模拟 Tiling：生成 [8, 36] 的输入流，但只裁剪出 [8, 16] 的有效输出。\n",
    "        input_tile_size: 决定输入数据流的行数 (6x6=36)\n",
    "        output_tile_size: 决定有效输出结果的行数 (4x4=16)\n",
    "        \"\"\"\n",
    "        inp_int, wgt_int = self.get_quantized_int()\n",
    "        \n",
    "        # 1. Slice: 取出 Input Channel 0-7, Output Channel 0-7\n",
    "        inp_slice = inp_int.squeeze(0)[:self.arr_size, :, :] # [8, 32, 32]\n",
    "        wgt_slice = wgt_int[:self.arr_size, :self.arr_size, :, :]\n",
    "        \n",
    "        # 2. 提取 input_tile_size x input_tile_size 的输入局部区域。\n",
    "        # [8, 32, 32] -> [8, 6, 6]\n",
    "        inp_tile_chw = inp_slice[:, :input_tile_size, :input_tile_size]\n",
    "        \n",
    "        # 3. 提取特定 kij 对应的权重 [8, 8]\n",
    "        ki = kij_idx // 3\n",
    "        kj = kij_idx % 3\n",
    "        w_matrix = wgt_slice[:, :, ki, kj] # [8, 8]\n",
    "        \n",
    "        # 4. 生成完整的 Input Stream (6x6 = 36 Time Steps)\n",
    "        full_inp_stream_list = []\n",
    "        \n",
    "        # r 和 c 循环的是 Input Tile 的尺寸 (6x6)，生成 36 个输入向量\n",
    "        for r in range(input_tile_size):  \n",
    "            for c in range(input_tile_size):  \n",
    "                # 采样位置：[channels, r, c]\n",
    "                input_vector = inp_tile_chw[:, r, c] # [8] 个输入通道的值\n",
    "                full_inp_stream_list.append(input_vector.unsqueeze(1))\n",
    "        \n",
    "        # Full Input Stream: [8 (In Ch), 36 (Time Steps)]\n",
    "        inp_stream_full = torch.cat(full_inp_stream_list, dim=1)\n",
    "        \n",
    "        # 5. 矩阵乘法模拟 (生成 36 个 Partial Sums)\n",
    "        # Output [8, 36] = Weight [8, 8] @ Input [8, 36]\n",
    "        full_output_int = torch.matmul(w_matrix.cuda(), inp_stream_full.cuda())\n",
    "        \n",
    "        # 6. 裁剪 Output (只保留 4x4 的有效区域)\n",
    "        # 假设 36 个输出是按照 R0C0..R0C5, R1C0..R1C5, ... 的顺序排列的\n",
    "        # 有效输出区域是 R1C1 到 R4C4 的 4x4 区域\n",
    "        \n",
    "        start_idx = input_tile_size + 1 # (1 * 6) + 1 = 7 (跳过 R0 和 R1C0)\n",
    "        valid_indices = []\n",
    "        \n",
    "        for r in range(output_tile_size): # R=0 to 3\n",
    "            # 在 6x6 的输出中，有效行是从 R=1 开始的，到 R=4 结束\n",
    "            # 在 6x6 的输出中，有效列是从 C=1 开始的，到 C=4 结束\n",
    "            \n",
    "            # 对应的 6x6 索引是： (r+1)*6 + (c+1)\n",
    "            base_idx = (r + 1) * input_tile_size + 1\n",
    "            for c in range(output_tile_size): # C=0 to 3\n",
    "                valid_indices.append(base_idx + c)\n",
    "\n",
    "        # 裁剪出 16 个有效的 Partial Sums\n",
    "        output_cropped = full_output_int[:, valid_indices] # [8, 16]\n",
    "        \n",
    "        print(f\"Software Verification (kij={kij_idx}): Input Stream Shape {inp_stream_full.shape}, Cropped Output Shape {output_cropped.shape}\")\n",
    "        \n",
    "        # 返回 36 行输入流和 16 行裁剪后的输出\n",
    "        return inp_stream_full, w_matrix, output_cropped\n",
    "\n",
    "\n",
    "    def export_files(self, kij_idx=0, prefix=\"test\", export_activation=True):\n",
    "        \"\"\"导出 .txt 文件\"\"\"\n",
    "        # Tiling 参数硬编码在 run_software_verification 内部，这里无需传入\n",
    "        inp_stream, w_matrix, out_ref = self.run_software_verification(kij_idx)\n",
    "        \n",
    "        # === 1. 导出 Activation (Input) === (36行)\n",
    "        tile_size = inp_stream.shape[1]\n",
    "        out_size = out_ref.shape[1]\n",
    "        act_filename = f\"{prefix}_activation_in{tile_size}.txt\" \n",
    "        if export_activation:\n",
    "            # ... (写入逻辑不变) ...\n",
    "            with open(act_filename, 'w') as f:\n",
    "                f.write('# [Time Step Major] Bits: Row7(MSB)..Row0(LSB) #\\n')\n",
    "                time_steps = inp_stream.shape[1]\n",
    "                rows = inp_stream.shape[0] # 8\n",
    "                \n",
    "                for t in range(time_steps):\n",
    "                    line_bin = \"\"\n",
    "                    for r in range(rows - 1, -1, -1): \n",
    "                        val = inp_stream[r, t].item()\n",
    "                        line_bin += self.int_to_bin_str(val, self.bits) \n",
    "                    f.write(line_bin + '\\n')\n",
    "            print(f\"Exported: {act_filename} ({time_steps} lines)\")\n",
    "\n",
    "        # === 2. 导出 Weight === (8行)\n",
    "        wgt_filename = f\"{prefix}_weight_kij{kij_idx}.txt\"\n",
    "        # ... (写入逻辑不变) ...\n",
    "        with open(wgt_filename, 'w') as f:\n",
    "            f.write('# [Col Major] Bits: Row7(InCh7)..Row0(InCh0) #\\n')\n",
    "            num_cols = w_matrix.shape[0] # Output Channels\n",
    "            num_rows = w_matrix.shape[1] # Input Channels\n",
    "            \n",
    "            for c in range(num_cols): # Output Channel Loop\n",
    "                line_bin = \"\"\n",
    "                for r in range(num_rows - 1, -1, -1): # Input Channel Loop (High to Low)\n",
    "                    val = w_matrix[c, r].item()\n",
    "                    line_bin += self.int_to_bin_str(val, self.bits)\n",
    "                f.write(line_bin + '\\n')\n",
    "        print(f\"Exported: {wgt_filename}\")\n",
    "        \n",
    "        # === 3. 导出 Golden Output (Partial Sum Reference) === (16行)\n",
    "        out_filename = f\"{prefix}_output_ref_kij{kij_idx}_out{out_size}.txt\"\n",
    "        # ... (写入逻辑不变) ...\n",
    "        with open(out_filename, 'w') as f:\n",
    "            for t in range(out_ref.shape[1]):\n",
    "                line_bin = \"\"\n",
    "                for r in range(out_ref.shape[0] - 1, -1, -1):\n",
    "                    val = out_ref[r, t].item()\n",
    "                    line_bin += self.int_to_bin_str(val, 16) \n",
    "                f.write(line_bin + '\\n')\n",
    "        print(f\"Exported: {out_filename}\")\n",
    "\n",
    "\n",
    "    def calculate_final_output(self, prefix=\"test\"):\n",
    "        \"\"\"计算并导出 9 个 kij Partial Sums 累加后的最终输出 (16行)\"\"\"\n",
    "        final_output = None\n",
    "        \n",
    "        print(\"Calculating final accumulated output...\")\n",
    "        \n",
    "        for kij_idx in range(9):\n",
    "            # 调用 run_software_verification (自动产生 16 行输出)\n",
    "            inp_stream, w_matrix, current_partial_sum = self.run_software_verification(kij_idx)\n",
    "            \n",
    "            if final_output is None:\n",
    "                final_output = current_partial_sum\n",
    "            else:\n",
    "                final_output += current_partial_sum\n",
    "            \n",
    "        # ... (Bias 和 ReLU 逻辑保持不变) ...\n",
    "        bias = self.layer.bias\n",
    "        if bias is not None:\n",
    "            bias_slice = bias[:self.arr_size].unsqueeze(1).expand_as(final_output)\n",
    "            final_output += bias_slice\n",
    "            print(\"Bias applied.\")\n",
    "        else:\n",
    "            print(\"Note: Target layer has no bias.\")\n",
    "        \n",
    "        # 转换为 16-bit 整数\n",
    "        final_output_int = torch.round(final_output)\n",
    "        final_output = torch.nn.functional.relu(final_output)\n",
    "        out_size = final_output_int.shape[1]\n",
    "\n",
    "        # 4. 导出 Final Output\n",
    "        out_filename = f\"{prefix}_final_output_ref_out{out_size}.txt\"\n",
    "        with open(out_filename, 'w') as f:\n",
    "            f.write('# [Final Accumulated Output] Bits: Row7(MSB)..Row0(LSB) #\\n')\n",
    "            \n",
    "            for t in range(final_output_int.shape[1]):\n",
    "                line_bin = \"\"\n",
    "                for r in range(final_output_int.shape[0] - 1, -1, -1):\n",
    "                    val = final_output_int[r, t].item()\n",
    "                    line_bin += self.int_to_bin_str(val, 16) \n",
    "                f.write(line_bin + '\\n')\n",
    "                \n",
    "        print(f\"Exported Final Output Reference: {out_filename} ({final_output_int.shape[1]} lines)\")\n",
    "        return final_output_int\n",
    "\n",
    "def train(model, trainloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(trainloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(trainloader)\n",
    "\n",
    "\n",
    "def test(model, testloader, criterion, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    return test_loss / len(testloader), correct / total\n",
    "\n",
    "def get_testloader(batch_size=128):\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    return testloader\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# 3. Hook：保存中间特征\n",
    "# -----------------------------------\n",
    "saved_features = {}   # 用来存储 hook 输出\n",
    "\n",
    "def save_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        # detach + to CPU + numpy（如果需要处理）\n",
    "        saved_features[name] = output.detach().cpu()\n",
    "    return hook\n",
    "\n",
    "print('All preparations finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a7a78d4-3933-4c39-87bd-e3e1b516964f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Files already downloaded and verified\n",
      "Captured input shape: torch.Size([1, 128, 16, 16])\n",
      "Quantization Info: Act_Scale=1.8229, Wgt_Scale=38.5760\n",
      "Software Verification (kij=0): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Exported: dense_layer1_conv2_new_activation_in36.txt (36 lines)\n",
      "Exported: dense_layer1_conv2_new_weight_kij0.txt\n",
      "Exported: dense_layer1_conv2_new_output_ref_kij0_out16.txt\n",
      "Software Verification (kij=1): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Exported: dense_layer1_conv2_new_activation_in36.txt (36 lines)\n",
      "Exported: dense_layer1_conv2_new_weight_kij1.txt\n",
      "Exported: dense_layer1_conv2_new_output_ref_kij1_out16.txt\n",
      "Software Verification (kij=2): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Exported: dense_layer1_conv2_new_activation_in36.txt (36 lines)\n",
      "Exported: dense_layer1_conv2_new_weight_kij2.txt\n",
      "Exported: dense_layer1_conv2_new_output_ref_kij2_out16.txt\n",
      "Software Verification (kij=3): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Exported: dense_layer1_conv2_new_activation_in36.txt (36 lines)\n",
      "Exported: dense_layer1_conv2_new_weight_kij3.txt\n",
      "Exported: dense_layer1_conv2_new_output_ref_kij3_out16.txt\n",
      "Software Verification (kij=4): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Exported: dense_layer1_conv2_new_activation_in36.txt (36 lines)\n",
      "Exported: dense_layer1_conv2_new_weight_kij4.txt\n",
      "Exported: dense_layer1_conv2_new_output_ref_kij4_out16.txt\n",
      "Software Verification (kij=5): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Exported: dense_layer1_conv2_new_activation_in36.txt (36 lines)\n",
      "Exported: dense_layer1_conv2_new_weight_kij5.txt\n",
      "Exported: dense_layer1_conv2_new_output_ref_kij5_out16.txt\n",
      "Software Verification (kij=6): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Exported: dense_layer1_conv2_new_activation_in36.txt (36 lines)\n",
      "Exported: dense_layer1_conv2_new_weight_kij6.txt\n",
      "Exported: dense_layer1_conv2_new_output_ref_kij6_out16.txt\n",
      "Software Verification (kij=7): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Exported: dense_layer1_conv2_new_activation_in36.txt (36 lines)\n",
      "Exported: dense_layer1_conv2_new_weight_kij7.txt\n",
      "Exported: dense_layer1_conv2_new_output_ref_kij7_out16.txt\n",
      "Software Verification (kij=8): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Exported: dense_layer1_conv2_new_activation_in36.txt (36 lines)\n",
      "Exported: dense_layer1_conv2_new_weight_kij8.txt\n",
      "Exported: dense_layer1_conv2_new_output_ref_kij8_out16.txt\n",
      "Calculating final accumulated output...\n",
      "Software Verification (kij=0): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Software Verification (kij=1): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Software Verification (kij=2): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Software Verification (kij=3): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Software Verification (kij=4): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Software Verification (kij=5): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Software Verification (kij=6): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Software Verification (kij=7): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Software Verification (kij=8): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Note: Target layer has no bias.\n",
      "Exported Final Output Reference: dense_layer1_conv2_relu_final_output_ref_out16.txt (16 lines)\n",
      "\n",
      "All files, including the final accumulated output reference, have been generated.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 第一步：复用你的旧代码加载模型和数据\n",
    "# ==========================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 1. 设置设备\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 2. 构建模型 (复用你的 build_model)\n",
    "model = build_model().to(device)\n",
    "\n",
    "# 3. 加载训练好的权重 (复用你的路径)\n",
    "weight_path = \"result/dense/best_densenet_cifar10.pth\"\n",
    "# 如果你有训练好的文件，取消下面注释\n",
    "# model.load_state_dict(torch.load(weight_path, map_location=device))\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# 4. 注册 Hook (复用你的逻辑，定位到想要导出的层)\n",
    "# 假设我们要导出 denseblock1 中 denselayer1 的 conv2 (3x3 卷积)\n",
    "target_layer = model.features.denseblock1.denselayer1.conv2\n",
    "capture_data = {}\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    capture_data['input'] = input[0].detach() # 保持在 GPU/CPU 均可，后续处理\n",
    "    capture_data['output'] = output.detach()\n",
    "\n",
    "handle = target_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "# ==========================================\n",
    "# 第二步：运行一次推理以捕获数据\n",
    "# ==========================================\n",
    "# 复用你的测试数据获取逻辑\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "])\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# 取一张图片，增加 batch 维度\n",
    "img, label = testset[0]\n",
    "img = img.unsqueeze(0).to(device)\n",
    "\n",
    "# 前向传播\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _ = model(img)\n",
    "\n",
    "# 移除 Hook\n",
    "handle.remove()\n",
    "print(f\"Captured input shape: {capture_data['input'].shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# 第三步：使用新工具导出 (这里嵌入我上一条回答提供的类)\n",
    "# ==========================================\n",
    "# 确保在这里粘贴了 class HardwareExporter ... 的定义代码\n",
    "\n",
    "# 初始化导出器\n",
    "# 注意：我们传入捕获到的 input 和 目标层 target_layer\n",
    "exporter = HardwareExporter(\n",
    "    layer=target_layer, \n",
    "    input_tensor=capture_data['input'], \n",
    "    bit_width=4, \n",
    "    array_size=8\n",
    ")\n",
    "\n",
    "# 导出文件\n",
    "# kij_idx=4 代表 3x3 卷积核的中心点 (中间的那个权重)\n",
    "# exporter.export_files(kij_idx=4, prefix=\"dense_layer1_conv2\")\n",
    "for i in range(0, 9):\n",
    "    exporter.export_files(kij_idx=i, prefix=\"dense_layer1_conv2_new\")\n",
    "final_output_tensor = exporter.calculate_final_output(prefix=\"dense_layer1_conv2_relu\")\n",
    "print(\"\\nAll files, including the final accumulated output reference, have been generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7ca24d8-9a9d-4a69-a24f-b2972222cbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating final accumulated output...\n",
      "Software Verification (kij=0): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Software Verification (kij=1): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Software Verification (kij=2): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Software Verification (kij=3): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Software Verification (kij=4): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Software Verification (kij=5): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Software Verification (kij=6): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Software Verification (kij=7): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Software Verification (kij=8): Input Stream Shape torch.Size([8, 36]), Cropped Output Shape torch.Size([8, 16])\n",
      "Note: Target layer has no bias.\n",
      "Exported Final Output Reference: dense_layer1_conv2_relu_final_output_ref_out16.txt (16 lines)\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "final_output_tensor = exporter.calculate_final_output(prefix=\"dense_layer1_conv2_relu\")\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3870939-d153-4e37-813f-6cadf69c7530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating full_acc_address.txt for 1024 output pixels (9216 total cycles)...\n",
      "Generation complete. The file 'full_acc_address.txt' contains 9216 11-bit addresses.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def generate_full_acc_address_file(output_width=32, output_height=32, address_bits=11, filename=\"full_acc_address.txt\"):\n",
    "    \"\"\"\n",
    "    根据用户提供的 acc_address.txt 示例文件中的规律，生成完整的累加器地址文件。\n",
    "\n",
    "    该示例文件展示了两个核心规律：\n",
    "    1. 相对地址模式 (Rj)：9个kij/MAC操作的相对地址。\n",
    "    2. 起始地址序列 (Sk)：每隔一个输出像素，起始地址有规律地跳变 (Sk = k + 2 * floor(k/4))。\n",
    "    \n",
    "    Args:\n",
    "        output_width (int): 目标输出特征图的宽度 (32)。\n",
    "        output_height (int): 目标输出特征图的高度 (32)。\n",
    "        address_bits (int): 累加器地址的位宽 (11)。\n",
    "        filename (str): 输出文件名。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 从用户提供的示例中提取 9 个 kij 的相对地址模式 (Rj)\n",
    "    # 转换为十进制，便于计算。这是 acc_address.txt 前 9 行的十进制值 。\n",
    "    RELATIVE_ADDRESSES = [\n",
    "        0,    # 00000000000\n",
    "        77,   # 00000100101\n",
    "        308,  # 00001001010\n",
    "        450,  # 00001110010\n",
    "        551,  # 00010010111\n",
    "        732,  # 00010111100\n",
    "        964,  # 00011100100\n",
    "        1089, # 00100001001\n",
    "        1278  # 00100101110\n",
    "    ]\n",
    "    \n",
    "    # 2. 定义起始地址序列的生成函数 (Sk)\n",
    "    # 规律：Sk = k + 2 * floor(k/4)\n",
    "    # 这表明您的硬件在处理4个像素后，会跳过2个地址，这很可能对应于您PSUM SRAM的Tiling结构。\n",
    "    def get_starting_address(k):\n",
    "        \"\"\"计算第 k 个输出像素的起始累加地址 (Sk)\"\"\"\n",
    "        return k + 2 * (k // 4)\n",
    "\n",
    "    # 3. 生成完整的地址文件\n",
    "    total_pixels = output_width * output_height # 32 * 32 = 1024\n",
    "    total_cycles = total_pixels * 9             # 1024 * 9 = 9216\n",
    "    \n",
    "    print(f\"Generating {filename} for {total_pixels} output pixels ({total_cycles} total cycles)...\")\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        # k: 循环所有输出像素 (从 0 到 1023)\n",
    "        for k in range(total_pixels):\n",
    "            \n",
    "            # S_k: 当前像素的起始地址\n",
    "            S_k = get_starting_address(k)\n",
    "            \n",
    "            # j: 循环 9 个 kij (从 0 到 8)\n",
    "            for j in range(9):\n",
    "                \n",
    "                # A: 绝对累加地址 = 起始地址 + 相对地址\n",
    "                A = S_k + RELATIVE_ADDRESSES[j]\n",
    "                \n",
    "                # 格式化为 11 位二进制字符串 (例如: 00000000000)\n",
    "                # 使用 format(number, '0b') 将整数转为二进制，并确保左侧补零\n",
    "                address_bin = f'{A:0{address_bits}b}'\n",
    "                \n",
    "                # 写入文件\n",
    "                f.write(address_bin + '\\n')\n",
    "                \n",
    "    print(f\"Generation complete. The file '{filename}' contains {total_cycles} 11-bit addresses.\")\n",
    "\n",
    "# 执行函数以生成文件\n",
    "generate_full_acc_address_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d37b639-224b-4262-beaf-7b5138195ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
