{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "VGG_quant(\n",
      "  (features): Sequential(\n",
      "    (0): QuantConv2d(\n",
      "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): QuantConv2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): QuantConv2d(\n",
      "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): QuantConv2d(\n",
      "      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): QuantConv2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantConv2d(\n",
      "      256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): QuantConv2d(\n",
      "      512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (31): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): QuantConv2d(\n",
      "      16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): QuantConv2d(\n",
      "      16, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): ReLU(inplace=True)\n",
      "    (39): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (41): ReLU(inplace=True)\n",
      "    (42): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (43): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"VGG16_squeezed\"\n",
    "model = VGG16_squeezed()\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [150, 225]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04771c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# --- 1. Set model save directory ---\n",
    "fdir = 'result/VGG16_quant'\n",
    "os.makedirs(fdir, exist_ok=True) # Ensure directory exists\n",
    "\n",
    "# --- 2. Move model to GPU and define loss function ---\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "# --- 3. Define training epochs and optimizer ---\n",
    "num_epochs = 300\n",
    "# Use AdamW optimizer for faster convergence\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-3, weight_decay=1e-2) \n",
    "\n",
    "# --- 4. Define learning rate scheduler ---\n",
    "# Use Cosine Annealing, T_max must equal total number of epochs\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "# --- 5. Start training loop ---\n",
    "best_prec = 0\n",
    "print(f\"==> Start training model for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # Validate model\n",
    "    prec = validate(testloader, model, criterion)\n",
    "    \n",
    "    # Call scheduler.step() after validate and before saving checkpoint\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Check if it is the best model\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec, best_prec)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)\n",
    "    \n",
    "    # Print current epoch learning rate and precision\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} Completed | Learning Rate: {current_lr:.6f} | Val Precision: {prec:.2f}% | Best Precision: {best_prec:.2f}%\")\n",
    "\n",
    "print(f\"==> Training completed. Best model saved to {fdir}/model_best.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_11556\\564645170.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 9025/10000 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/VGG16_quant/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf88ba6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for the index of the 16x16 convolution layer...\n",
      "âœ… Target layer found! Index = 34\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Locate the target layer ---\n",
    "print(\"Searching for the index of the 16x16 convolution layer...\")\n",
    "target_layer_idx = -1\n",
    "for i, layer in enumerate(model.features):\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        if layer.in_channels == 16 and layer.out_channels == 16:\n",
    "            target_layer_idx = i\n",
    "            print(f\"âœ… Target layer found! Index = {i}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f219b7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction completed!\n",
      "Activation Shape: torch.Size([128, 16, 2, 2])\n",
      "Weight Shape: torch.Size([16, 16, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Capture and Extract Quantized Data (Hook & Extract) ---\n",
    "\n",
    "# A. Prepare Hook\n",
    "inputs_q = []\n",
    "def hook(module, input, output):\n",
    "    # input[0] is the Activation we want\n",
    "    inputs_q.append(input[0].detach().cpu())\n",
    "\n",
    "# B. Register Hook to the layer found earlier\n",
    "target_layer = model.features[target_layer_idx] # Use the index found earlier\n",
    "handle = target_layer.register_forward_hook(hook)\n",
    "\n",
    "# C. Run one forward pass (Inference)\n",
    "# Take one Batch from the test loader\n",
    "data, _ = next(iter(testloader)) \n",
    "data = data.cuda()\n",
    "model(data) # Trigger Hook\n",
    "\n",
    "# D. Remove Hook\n",
    "handle.remove()\n",
    "\n",
    "# E. Process extracted data\n",
    "# 1. Process Activation (2-bit)\n",
    "act = inputs_q[0] # Retrieve input of the first Batch\n",
    "act_alpha = target_layer.act_alpha.detach().cpu()\n",
    "act_bit = 2 # [Key] Force set to 2-bit\n",
    "# Quantization formula: float -> int\n",
    "act_q = model.features[target_layer_idx].act_alq(act.cuda(), act_alpha.cuda()).cpu()\n",
    "act_int = act_q / (act_alpha / (2**act_bit - 1))\n",
    "act_int = act_int.round() # Ensure it is an integer 0, 1, 2, 3\n",
    "\n",
    "# 2. Process Weight (4-bit) - Revised Version\n",
    "# Directly get the quantized weights (Fake Quantized Float) calculated and stored during forward\n",
    "# Note: This requires having run model(data) earlier, because self.weight_q is updated in forward\n",
    "weight_fake_quant = target_layer.weight_q.detach().cpu() \n",
    "\n",
    "wgt_alpha = target_layer.weight_quant.wgt_alpha.detach().cpu()\n",
    "w_bit = 4\n",
    "\n",
    "# Derive Integer: Integer = FakeFloat / Scale\n",
    "# Scale = alpha / (2^(b-1) - 1)\n",
    "scale = wgt_alpha / (2**(w_bit-1) - 1)\n",
    "\n",
    "# Divide by scale directly to get integer\n",
    "weight_int = weight_fake_quant / scale\n",
    "\n",
    "# Round to ensure it is an integer (eliminate floating point error)\n",
    "weight_int = weight_int.round()\n",
    "\n",
    "# Clamp between -7 and 7 (4-bit signed range: -8 to 7, but symmetric quantization is usually -7 to 7)\n",
    "# Check uniform_quant implementation in quant_layer.py, it uses clamp(min=-1, max=1)\n",
    "# So integer range should be -(2^(b-1)-1) to +(2^(b-1)-1), i.e., -7 to +7\n",
    "weight_int = weight_int.clamp(-(2**(w_bit-1)-1), (2**(w_bit-1)-1))\n",
    "\n",
    "print(\"Data extraction completed!\")\n",
    "print(f\"Activation Shape: {act_int.shape}\")\n",
    "print(f\"Weight Shape: {weight_int.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61d56fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Generating V3 Hardware Test Data (Using Real Extracted Data)...\n",
      "Activation Shape Used: torch.Size([1, 16, 2, 2])\n",
      "Weight Shape Used: torch.Size([16, 16, 3, 3])\n",
      " -> Writing activationSIMD.txt ...\n",
      " -> Writing weightSIMD.txt (Generating weights for IC 0 & 1 only)...\n",
      " -> Calculating Reference Output (Simulating Partial Execution)...\n",
      "âœ… Done! Real data generated, and Reference calibrated for 36-line weight truncation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"ðŸš€ Generating V3 Hardware Test Data (Using Real Extracted Data)...\")\n",
    "\n",
    "# --- 1. Data Preparation and Slicing ---\n",
    "# Ensure data dimensions match requirements: IC=16, OC=16\n",
    "# act_int original shape might be [1, 64, H, W], weight_int is [64, 64, 3, 3]\n",
    "# We force slice to take the first 16 channels\n",
    "\n",
    "# [Batch, IC, H, W] -> Take Batch 0, IC 0-16\n",
    "real_act = act_int[0:1, 0:16, :, :].float() \n",
    "# [OC, IC, K, K] -> Take OC 0-16, IC 0-16\n",
    "real_weight = weight_int[0:16, 0:16, :, :].float()\n",
    "\n",
    "print(f\"Activation Shape Used: {real_act.shape}\")\n",
    "print(f\"Weight Shape Used: {real_weight.shape}\")\n",
    "\n",
    "# Padding to 4x4 (VGG usually has padding=1, 2x2 input becomes 4x4 after padding)\n",
    "act_padded = F.pad(real_act, (1, 1, 1, 1), \"constant\", 0) \n",
    "# Flatten: [1, 16, 4, 4] -> [1, 16, 16] (Time step)\n",
    "act_flat = act_padded.view(1, 16, -1) \n",
    "\n",
    "def decimal_to_hex(val, width=4):\n",
    "    val = int(val)\n",
    "    if val < 0: val = (1 << width) + val\n",
    "    return f'{val & ((1<<width)-1):01x}'\n",
    "\n",
    "# --- 2. Activation Packing (16 lines) ---\n",
    "# The logic here is correct: it packs data from 16 Input Channels within one time step\n",
    "# r=0 -> IC 0,1; r=7 -> IC 14,15\n",
    "print(f\" -> Writing activationSIMD.txt ...\")\n",
    "with open('activationSIMD.txt', 'w') as f:\n",
    "    for t in range(16): # 16 time steps (Pixels)\n",
    "        line_hex = \"\"\n",
    "        for r in range(7, -1, -1): # 8 Rows, each Row contains 2 Channels\n",
    "            ch_low = 2 * r\n",
    "            ch_high = 2 * r + 1\n",
    "            \n",
    "            # Get value from real data\n",
    "            val_low = act_flat[0, ch_low, t].item()\n",
    "            val_high = act_flat[0, ch_high, t].item()\n",
    "            \n",
    "            # 2-bit Packing\n",
    "            packed_val = (int(val_high) << 2) | (int(val_low) & 0b11)\n",
    "            line_hex += decimal_to_hex(packed_val, 4)\n",
    "        f.write(line_hex + '\\n')\n",
    "\n",
    "# --- 3. Weight Packing (36 lines - Only IC 0 & 1) ---\n",
    "print(f\" -> Writing weightSIMD.txt (Generating weights for IC 0 & 1 only)...\")\n",
    "with open('weightSIMD.txt', 'w') as f:\n",
    "    for tile in range(2): # 2 Output Channel Tiles (0-7, 8-15)\n",
    "        start_oc = tile * 8\n",
    "        for ky in range(3):\n",
    "            for kx in range(3):\n",
    "                # Low 4-bits: Weights corresponding to IC 0\n",
    "                line_low = \"\"\n",
    "                for oc_idx in range(7, -1, -1):\n",
    "                    out_c = start_oc + oc_idx\n",
    "                    # Get real weights (IC=0)\n",
    "                    w_val = real_weight[out_c, 0, ky, kx].item() \n",
    "                    line_low += decimal_to_hex(w_val, 4)\n",
    "                f.write(line_low + '\\n')\n",
    "                \n",
    "                # High 4-bits: Weights corresponding to IC 1\n",
    "                line_high = \"\"\n",
    "                for oc_idx in range(7, -1, -1):\n",
    "                    out_c = start_oc + oc_idx\n",
    "                    # Get real weights (IC=1)\n",
    "                    w_val = real_weight[out_c, 1, ky, kx].item()\n",
    "                    line_high += decimal_to_hex(w_val, 4)\n",
    "                f.write(line_high + '\\n')\n",
    "\n",
    "# --- 4. Output Ref (Revised Version) ---\n",
    "# Critical Fix: To match hardware behavior, when calculating the Reference here,\n",
    "# we must simulate the hardware scenario where \"only IC 0 and IC 1 weights are loaded\".\n",
    "# So we manually set weights for IC 2 to 15 to zero, then perform convolution.\n",
    "\n",
    "weight_simulated = real_weight.clone()\n",
    "weight_simulated[:, 2:, :, :] = 0 # [Key] Set IC 2-15 weights to 0 because they were not written to the file\n",
    "\n",
    "print(\" -> Calculating Reference Output (Simulating Partial Execution)...\")\n",
    "# Execute Convolution + ReLU\n",
    "out_ref = F.relu(F.conv2d(act_padded, weight_simulated, padding=0))\n",
    "\n",
    "with open('output_ref_SIMD.txt', 'w') as f:\n",
    "    H_out, W_out = out_ref.shape[2], out_ref.shape[3]\n",
    "    for h in range(H_out):\n",
    "        for w in range(W_out):\n",
    "            for tile in range(2):\n",
    "                start_oc = tile * 8\n",
    "                line = \"\"\n",
    "                for oc_idx in range(7, -1, -1):\n",
    "                    val = int(out_ref[0, start_oc+oc_idx, h, w].item())\n",
    "                    line += decimal_to_hex(val, 4)\n",
    "                f.write(line + '\\n')\n",
    "\n",
    "# Rewrite output writer logic just in case (adapt for psum_bw=16)\n",
    "def decimal_to_hex_safe(val, bits=16):\n",
    "    val = int(val)\n",
    "    if val < 0: val = (1 << bits) + val\n",
    "    return f'{val & ((1<<bits)-1):04x}' # 4 hex chars for 16 bits\n",
    "\n",
    "with open('output_ref_SIMD.txt', 'w') as f:\n",
    "    H_out, W_out = out_ref.shape[2], out_ref.shape[3]\n",
    "    for h in range(H_out):\n",
    "        for w in range(W_out):\n",
    "            for tile in range(2):\n",
    "                start_oc = tile * 8\n",
    "                line = \"\"\n",
    "                for oc_idx in range(7, -1, -1):\n",
    "                    val = int(out_ref[0, start_oc+oc_idx, h, w].item())\n",
    "                    line += decimal_to_hex_safe(val, 16) # Use 16-bit width\n",
    "                f.write(line + '\\n')\n",
    "\n",
    "print(\"âœ… Done! Real data generated, and Reference calibrated for 36-line weight truncation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
